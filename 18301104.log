6.30

创建了两个git仓库
分别是前端和后端
给大家创建了各自的分支
配置了python环境，spark，hadoop
下载了python包



现在卡在了虚拟机步骤，我的虚拟机里面，不能执行/etc/init.d/networking指令，显示未知指令，我还在努力找解决方案


7.1 上午

成功搭建了虚拟机，但是我没有按照pdf步骤配置手动nat地址分配，我是自动分配的

现在我的winscp能够正常传输文件，xshell能够连接到虚拟机

现在的问题是我不明白主节点和从节点是什么意思？老师能下午讲讲吗？
节点那块我没有配置，直接先安装jdk和hadoop了。jdk我用的是虚拟机的命令行直接安装的openjdk，因为我传过去的文件按照pdf配置好环境变量后，使用source后，输入java -version显示 "No such file or directory"


7.1下午
成功的在虚拟机安装了jdk、hadoop、spark。我明白了主节点和从节点就是三个虚拟机，我完成了三个虚拟机之间的互通。现在停留在了对hadoop测试阶段。预计明天上午可以完成整个环境配置并对数据开始进行清洗。

问题就是，我们组内不清楚后面的大部分任务都是干什么的，到底是前端部分还是后端部分？我们没办法进行进一步的分工，所以打算明天开会讨论每个任务都是干什么的。希望老师能讲解下每个任务的内容和目标


7.2全天

我边下载那60多g的数据，边学习python的基础语法。同时我还努力了解ARIMA模型，时间序列算法。

经过了连着三天的努力摸索，我们终于明白虚拟机只需要一个人搭建就行了，而且还是附属内容。哭了。。。
然后我们中午就讨论了下，借鉴了别的组的任务分配，大概搞清楚了前端和后端共同的，分开的任务。所以我们今天下午小组组员都在学习我说的那些。



7.3上午

在pycharm中配置好了spark（因为命令行的spark实在是太麻烦了），成功对数据进行了清洗
学习了ARIMA模型


7.3下午

下午进度有点慢了，我还是不太能搞懂ARIMA模型和时间序列的相关知识，晚上得加班学学了。。。但是我顺手把python相关的一些问题解决了